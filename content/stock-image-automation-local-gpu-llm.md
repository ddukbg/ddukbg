---
title: "ë¡œì»¬ GPUì™€ LLMì„ í™œìš©í•œ ìŠ¤í†¡ ì´ë¯¸ì§€ ìƒì„± ìë™í™” ì‹œìŠ¤í…œ êµ¬ì¶•í•˜ê¸°"
date: "2025-06-01 17:59:00"
description: "ë¡œì»¬ GPUì™€ LLMì„ ì‚¬ìš©í•´ ìŠ¤í†¡ ì´ë¯¸ì§€ ìƒì„± ìë™í™” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•œ ê²½í—˜ê³¼ êµ¬í˜„ ê³¼ì •ì„ ê³µìœ í•©ë‹ˆë‹¤."
---

# ë¡œì»¬ GPUì™€ LLMì„ í™œìš©í•œ ìŠ¤í†¡ ì´ë¯¸ì§€ ìƒì„± ìë™í™” ì‹œìŠ¤í…œ êµ¬ì¶• ê²½í—˜ê¸°

## ëª©ì°¨
1. [í”„ë¡œì íŠ¸ ë°°ê²½](#í”„ë¡œì íŠ¸-ë°°ê²½)
2. [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì„¤ê³„](#ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜-ì„¤ê³„)
3. [í•µì‹¬ êµ¬í˜„ ì‚¬í•­](#í•µì‹¬-êµ¬í˜„-ì‚¬í•­)
4. [ë™ì  í”„ë¡¬í”„íŠ¸ ìƒì„±ì˜ í•„ìš”ì„±ê³¼ êµ¬í˜„](#ë™ì -í”„ë¡¬í”„íŠ¸-ìƒì„±ì˜-í•„ìš”ì„±ê³¼-êµ¬í˜„)
5. [ë‹¤ì¤‘ ëª¨ë¸ ì§€ì› ë° í´ë°± ì „ëµ](#ë‹¤ì¤‘-ëª¨ë¸-ì§€ì›-ë°-í´ë°±-ì „ëµ)
6. [ë¡œì»¬ vs í´ë¼ìš°ë“œ ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ](#ë¡œì»¬-vs-í´ë¼ìš°ë“œ-ëª¨ë¸-ì„ íƒ-ê°€ì´ë“œ)
7. [ì‹¤ì œ êµ¬í˜„ì—ì„œ ë§ˆì£¼í•œ ë¬¸ì œë“¤ê³¼ í•´ê²°ì±…](#ì‹¤ì œ-êµ¬í˜„ì—ì„œ-ë§ˆì£¼í•œ-ë¬¸ì œë“¤ê³¼-í•´ê²°ì±…)
8. [ì„±ëŠ¥ ìµœì í™” ê²½í—˜](#ì„±ëŠ¥-ìµœì í™”-ê²½í—˜)
9. [í”„ë¡œì íŠ¸ íšŒê³  ë° ê°œì„  ë°©í–¥](#í”„ë¡œì íŠ¸-íšŒê³ -ë°-ê°œì„ -ë°©í–¥)

## í”„ë¡œì íŠ¸ ë°°ê²½

ë¯¸ë¦¬ìº”ë²„ìŠ¤ë‚˜ Adobe Stock ê°™ì€ í”Œë«í¼ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë””ìì¸ ì—ì…‹ì„ ëŒ€ëŸ‰ìœ¼ë¡œ ìƒì„±í•´ì„œ ë¶€ê°€ì ì¸ ìˆ˜ìµì„ ì°½ì¶œí• ìˆ˜ ìˆëŠ” ì‚¬ì‹¤ ì•Œê³ ê³„ì‹œë‚˜ìš”? í•˜ì§€ë§Œ ì´ë¯¸ì§€ ìƒì„±ì— ì‚¬ìš©ë˜ëŠ” API ë¹„ìš©ì€ ë§ì€ ë¶€ë‹´ìš”ì†Œì…ë‹ˆë‹¤. ê·¸ë˜ì„œ ì €ëŠ” ë¶€ë‹´ ì—†ì´ ì‘ì—…í•˜ê³  ì‹¶ì—ˆê³ , ë¡œì»¬ GPU ìì›ì„ ìµœëŒ€í•œ í™œìš©í•´ë³´ê³  ì‹¶ì—ˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ì—¬ëŸ¬ ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ì„ í†µí•©í•˜ê³ , í”„ë¡¬í”„íŠ¸ ìƒì„±ë¶€í„° ë©”íƒ€ë°ì´í„° ê´€ë¦¬ê¹Œì§€ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ìë™í™”í•˜ëŠ” ì‹œìŠ¤í…œì„ ë§Œë“¤ì–´ë³´ì•˜ìŠµë‹ˆë‹¤.

## ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì„¤ê³„

ì „ì²´ ì‹œìŠ¤í…œì„ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¡œ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤:

```mermaid
graph TD
    A[ì‚¬ìš©ì ì…ë ¥] --> B{í”„ë¡¬í”„íŠ¸ ëª¨ë“œ ì„ íƒ}
    B --> |ëœë¤| C[ì‚¬ì „ ì •ì˜ í…œí”Œë¦¿ ì¡°í•©]
    B --> |í‚¤ì›Œë“œ| D[í‚¤ì›Œë“œ ê¸°ë°˜ í™•ì¥]
    B --> |ë™ì | E[Ollama LLM ìƒì„±]
    
    C & D & E --> F[í”„ë¡¬í”„íŠ¸ ì •ì œ]
    F --> G{ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ ì„ íƒ}
    
    G --> |ë¡œì»¬| H[ë¡œì»¬ GPU ëª¨ë¸]
    G --> |API| I[í´ë¼ìš°ë“œ API ëª¨ë¸]
    
    H --> J[SDXL + Refiner]
    H --> K[Flux.1 Schnell]
    
    I --> L[DALL-E 3]
    I --> M[Google Gemini/Imagen]
    
    J & K & L & M --> N[ì´ë¯¸ì§€ í›„ì²˜ë¦¬]
    N --> O[ë©”íƒ€ë°ì´í„° ìƒì„±]
    O --> P[íŒŒì¼ ì €ì¥ ë° ê´€ë¦¬]
    
    style A fill:#e1f5fe
    style H fill:#c8e6c9
    style I fill:#ffccbc
    style P fill:#f3e5f5
```

## í•µì‹¬ êµ¬í˜„ ì‚¬í•­

### ëª¨ë“ˆí™”ëœ êµ¬ì¡° ì„¤ê³„

ê° ê¸°ëŠ¥ì„ ë…ë¦½ì ì¸ í•¨ìˆ˜ë¡œ ë¶„ë¦¬í•˜ì—¬ ìœ ì§€ë³´ìˆ˜ì™€ í™•ì¥ì´ ìš©ì´í•˜ë„ë¡ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤:

```python
# í”„ë¡¬í”„íŠ¸ ìƒì„± ëª¨ë“ˆ
def generate_prompt(base_idea=None):
    """í‚¤ì›Œë“œ ê¸°ë°˜ ë˜ëŠ” ëœë¤ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    # êµ¬í˜„ ë‚´ìš©...

def generate_dynamic_prompt_with_ollama():
    """Ollamaë¥¼ í™œìš©í•œ ë™ì  í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    # êµ¬í˜„ ë‚´ìš©...

# ì´ë¯¸ì§€ ìƒì„± ëª¨ë“ˆ
def generate_image_with_local(prompt_data):
    """ë¡œì»¬ SDXL ëª¨ë¸ í™œìš©"""
    # êµ¬í˜„ ë‚´ìš©...

def generate_image_with_flux(prompt_data):
    """ë¡œì»¬ Flux ëª¨ë¸ í™œìš©"""
    # êµ¬í˜„ ë‚´ìš©...

# í›„ì²˜ë¦¬ ëª¨ë“ˆ
def process_image(image, prompt_data, format_type):
    """ì´ë¯¸ì§€ í›„ì²˜ë¦¬ ë° ë©”íƒ€ë°ì´í„° ìƒì„±"""
    # êµ¬í˜„ ë‚´ìš©...
```

### ì„¤ì • ê¸°ë°˜ ìœ ì—°ì„± í™•ë³´

í™˜ê²½ ë³€ìˆ˜ì™€ ì„¤ì •ì„ í†µí•´ ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ë™ì‘í•  ìˆ˜ ìˆë„ë¡ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤:

```python
# í™˜ê²½ë³„ ì„¤ì •
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
OLLAMA_API_URL = "http://localhost:11434/api/generate"
OLLAMA_MODEL = "qwen2.5:14b"  # ì„¤ì • ê°€ëŠ¥í•œ ëª¨ë¸

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒì  importë¡œ ì˜ì¡´ì„± ë¬¸ì œ í•´ê²°
try:
    from rembg import remove
    REMBG_AVAILABLE = True
except ImportError:
    print("ê²½ê³ : ë°°ê²½ ì œê±° ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
    REMBG_AVAILABLE = False
```

## ë™ì  í”„ë¡¬í”„íŠ¸ ìƒì„±ì˜ í•„ìš”ì„±ê³¼ êµ¬í˜„

### ì™œ ë™ì  í”„ë¡¬í”„íŠ¸ê°€ í•„ìš”í•œê°€?

ê¸°ì¡´ì˜ ëœë¤ ì¡°í•©ì´ë‚˜ í‚¤ì›Œë“œ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„± ë°©ì‹ì€ ë‹¤ìŒê³¼ ê°™ì€ í•œê³„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤:

1. **ì œí•œëœ ì°½ì˜ì„±**: ì‚¬ì „ ì •ì˜ëœ í…œí”Œë¦¿ ì¡°í•©ìœ¼ë¡œëŠ” ì°¸ì‹ í•œ ì•„ì´ë””ì–´ ìƒì„±ì´ ì–´ë ¤ì›€
2. **ë°˜ë³µì ì¸ ê²°ê³¼**: ë¹„ìŠ·í•œ íŒ¨í„´ì˜ ì´ë¯¸ì§€ê°€ ê³„ì† ìƒì„±ë¨
3. **ë§¥ë½ ì´í•´ ë¶€ì¡±**: ë‹¨ìˆœ í‚¤ì›Œë“œ ì¡°í•©ìœ¼ë¡œëŠ” ë³µí•©ì ì¸ ê°œë… í‘œí˜„ì´ ì–´ë ¤ì›€

### ì‹¤ì œ êµ¬í˜„ ë°©ì‹

Ollama LLMì„ í™œìš©í•˜ì—¬ êµ¬ì¡°í™”ëœ JSON í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ë„ë¡ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤:

```python
def generate_dynamic_prompt_with_ollama():
    master_prompt = f"""
You are a creative assistant specialized in generating ideas for design assets.
Your task is to generate ONE creative concept for a design asset.

Output the result ONLY in JSON format with the following exact keys:
{{
  "korean_description": "<ìƒì„±ëœ ì—ì…‹ ì•„ì´ë””ì–´ì— ëŒ€í•œ ê°„ê²°í•œ í•œê¸€ ì„¤ëª…>",
  "english_image_prompt": "<Detailed English prompt for image generation>",
  "tags": ["<List of 5-7 relevant Korean and English tags>"]
}}

Generate a new, creative concept now.
"""
    
    try:
        payload = {
            "model": OLLAMA_MODEL,
            "prompt": master_prompt,
            "format": "json",
            "stream": False
        }
        response = requests.post(OLLAMA_API_URL, json=payload, timeout=120)
        
        # JSON íŒŒì‹± ë° ê²€ì¦ ë¡œì§
        parsed_result = json.loads(response.json().get("response"))
        
        return {
            "prompt": parsed_result.get("english_image_prompt"),
            "korean_prompt_base": parsed_result.get("korean_description"),
            "tags": parsed_result.get("tags", [])
        }
    except Exception as e:
        # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸°ë¡œ í´ë°±
        return generate_prompt()
```

### ë™ì  í”„ë¡¬í”„íŠ¸ ìƒì„± ì˜ˆì‹œ

**ì…ë ¥**: "ìƒˆë¡œìš´ ë””ìì¸ ì—ì…‹ ì•„ì´ë””ì–´ë¥¼ ë§Œë“¤ì–´ì¤˜"

**Ollama ì¶œë ¥**:
```json
{
  "korean_description": "ìš°ì£¼ ì •ê±°ì¥ì—ì„œ ë°”ë¼ë³¸ ì§€êµ¬ì˜ ì¼ì¶œ í’ê²½",
  "english_image_prompt": "Spectacular sunrise over Earth as seen from space station window, cinematic lighting, detailed planet surface, realistic space environment, high contrast, professional space photography style",
  "tags": ["ìš°ì£¼", "ì§€êµ¬", "ì¼ì¶œ", "space", "earth", "sunrise", "cinematic"]
}
```

ì´ì²˜ëŸ¼ ë‹¨ìˆœí•œ í‚¤ì›Œë“œ ì¡°í•©ìœ¼ë¡œëŠ” ë§Œë“¤ì–´ë‚´ê¸° ì–´ë ¤ìš´ ì°½ì˜ì ì´ê³  êµ¬ì²´ì ì¸ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

## ë‹¤ì¤‘ ëª¨ë¸ ì§€ì› ë° í´ë°± ì „ëµ

### êµ¬í˜„í•œ í´ë°± ì‹œìŠ¤í…œ

ê° ëª¨ë¸ì´ ì‹¤íŒ¨í•  ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë‹¨ê³„ì  í´ë°± ì‹œìŠ¤í…œì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤:

```python
def test_image_generation(count=1, model="local", user_prompt=None, prompt_mode="random"):
    # ëª¨ë¸ë³„ ì‹¤í–‰ ë° í´ë°± ë¡œì§
    if selected_model == "gemini" and GOOGLE_API_KEY:
        image, prompt_data = generate_image_with_gemini(prompt_data)
        if image is None: 
            actual_model_used = "dalle"
    elif selected_model == "dalle" and OPENAI_API_KEY:
        image, prompt_data = generate_image_with_dalle(prompt_data)
        if image is None: 
            actual_model_used = "random"
    elif selected_model == "local":
        if DIFFUSERS_AVAILABLE:
            image, prompt_data = generate_image_with_local(prompt_data)
        else:
            # ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜ ì‹œ API ëª¨ë¸ë¡œ ëŒ€ì²´
            if OPENAI_API_KEY: 
                actual_model_used = "dalle"
                image, prompt_data = generate_image_with_dalle(prompt_data)
    # ìµœì¢… ì‹¤íŒ¨ ì‹œ ëœë¤ ìƒ‰ìƒ ì´ë¯¸ì§€ ìƒì„± (ì™„ì „í•œ ì‹¤íŒ¨ ë°©ì§€)
    else:
        color = (random.randint(200, 255), random.randint(200, 255), random.randint(200, 255))
        image = Image.new('RGB', (1024, 1024), color=color)
```

### GPU ë©”ëª¨ë¦¬ ìµœì í™”

ë¡œì»¬ GPUë¥¼ ì‚¬ìš©í•  ë•Œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì´ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤:

```python
def generate_image_with_local(prompt_data):
    # ì¥ì¹˜ ë° ë°ì´í„° íƒ€ì… ìë™ ê°ì§€
    if torch.cuda.is_available():
        device = torch.device("cuda")
        torch_dtype = torch.float16  # GPU ë©”ëª¨ë¦¬ ì ˆì•½
        print("ğŸš€ CUDA ê°€ì† (float16) ì‚¬ìš©")
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        device = torch.device("mps")
        torch_dtype = torch.float16  # Mac GPU ì§€ì›
        print("ğŸš€ MPS ê°€ì† (float16) ì‚¬ìš©")
    else:
        device = torch.device("cpu")
        torch_dtype = torch.float32
        print("âš ï¸ CPU ì‚¬ìš© (ì†ë„ ì €í•˜ ì˜ˆìƒ)")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”ë¥¼ ìœ„í•œ ì„¤ì •
    pipe = StableDiffusionXLPipeline.from_pretrained(
        base_model_id, 
        torch_dtype=torch_dtype, 
        variant="fp16",  # ì ˆë°˜ ì •ë°€ë„ ëª¨ë¸ ì‚¬ìš©
        use_safetensors=True
    )
```

## ë¡œì»¬ vs í´ë¼ìš°ë“œ ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ

ì‹¤ì œ ì‚¬ìš© ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ê° ëª¨ë¸ì˜ íŠ¹ì„±ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤:

### ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©ì„ ê¶Œì¥í•˜ëŠ” ê²½ìš°

**ì¥ì :**
- ë¬´ì œí•œ ìƒì„± (API ë¹„ìš© ì—†ìŒ)
- ë°ì´í„° í”„ë¼ì´ë²„ì‹œ ë³´ì¥
- ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë¶ˆí•„ìš”
- ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥

**ì í•©í•œ ìƒí™©:**
- ëŒ€ëŸ‰ ì´ë¯¸ì§€ ìƒì„±ì´ í•„ìš”í•œ ê²½ìš°
- ì˜ˆì‚°ì´ ì œí•œì ì¸ ê°œì¸/ì†Œê·œëª¨ í”„ë¡œì íŠ¸
- ë°ì´í„° ë³´ì•ˆì´ ì¤‘ìš”í•œ í”„ë¡œì íŠ¸
- ì‹¤í—˜ì ì¸ ì‘ì—…ì´ ë§ì€ ê²½ìš°

### í´ë¼ìš°ë“œ API ëª¨ë¸ì„ ê¶Œì¥í•˜ëŠ” ê²½ìš°

**ì¥ì :**
- ë¹ ë¥¸ ìƒì„± ì†ë„
- ë†’ì€ í’ˆì§ˆê³¼ ì¼ê´€ì„±
- ì¸í”„ë¼ ê´€ë¦¬ ë¶ˆí•„ìš”
- ìµœì‹  ëª¨ë¸ ì§€ì† ì—…ë°ì´íŠ¸

**ì í•©í•œ ìƒí™©:**
- ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ì´ í•„ìš”í•œ ê²½ìš°
- ë†’ì€ í’ˆì§ˆì´ ìš°ì„ ì¸ ìƒì—…ì  í”„ë¡œì íŠ¸
- GPU í•˜ë“œì›¨ì–´ê°€ ì—†ëŠ” í™˜ê²½
- ì†ŒëŸ‰ì˜ ê³ í’ˆì§ˆ ì´ë¯¸ì§€ê°€ í•„ìš”í•œ ê²½ìš°

### ì‹¤ì œ ì„±ëŠ¥ ë¹„êµ (ê°œì¸ ê²½í—˜)

| í•­ëª© | ë¡œì»¬ SDXL | ë¡œì»¬ Flux | DALL-E 3 | Google Gemini |
|------|-----------|-----------|----------|---------------|
| ìƒì„± ì†ë„ | 40-60ì´ˆ | 15-20ì´ˆ | 10-15ì´ˆ | 8-12ì´ˆ |
| í’ˆì§ˆ | ì¢‹ìŒ | ë§¤ìš° ì¢‹ìŒ | ìš°ìˆ˜ | ìš°ìˆ˜ |
| ë¹„ìš© | ì „ê¸°ë£Œë§Œ | ì „ê¸°ë£Œë§Œ | $0.04/ì¥ | ë¬´ë£Œ (ì œí•œì ) |
| ì»¤ìŠ¤í„°ë§ˆì´ì§• | ë†’ìŒ | ì¤‘ê°„ | ë‚®ìŒ | ë‚®ìŒ |

*í…ŒìŠ¤íŠ¸ í™˜ê²½: RTX 3060 12GB, 16GB RAM*

## ì‹¤ì œ êµ¬í˜„ì—ì„œ ë§ˆì£¼í•œ ë¬¸ì œë“¤ê³¼ í•´ê²°ì±…

### 1. GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ

**ë¬¸ì œ**: SDXL + Refiner ëª¨ë¸ ë™ì‹œ ë¡œë”© ì‹œ 12GB GPU ë©”ëª¨ë¦¬ ì´ˆê³¼

**í•´ê²°ì±…**: 
```python
# ë‹¨ê³„ë³„ ë©”ëª¨ë¦¬ ê´€ë¦¬
base_model = load_base_model()
latents = base_model.generate()
del base_model  # ë² ì´ìŠ¤ ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ
torch.cuda.empty_cache()  # GPU ìºì‹œ ì •ë¦¬

refiner_model = load_refiner_model()
final_image = refiner_model.refine(latents)
```

### 2. Ollama JSON íŒŒì‹± ì‹¤íŒ¨

**ë¬¸ì œ**: LLMì´ ë•Œë¡œëŠ” ìœ íš¨í•˜ì§€ ì•Šì€ JSONì„ ë°˜í™˜

**í•´ê²°ì±…**:
```python
try:
    parsed_result = json.loads(generated_json_str)
except json.JSONDecodeError as json_err:
    logging.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {json_err}")
    # ì •ê·œì‹ìœ¼ë¡œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ ì‹œë„
    json_match = re.search(r'\{.*\}', generated_json_str, re.DOTALL)
    if json_match:
        parsed_result = json.loads(json_match.group())
    else:
        # ì™„ì „ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸°ë¡œ í´ë°±
        return generate_prompt()
```

### 3. ë‹¤ì–‘í•œ ì´ë¯¸ì§€ í•´ìƒë„ ëŒ€ì‘

**ë¬¸ì œ**: í”Œë«í¼ë³„ë¡œ ìš”êµ¬í•˜ëŠ” ìµœì†Œ í•´ìƒë„ê°€ ë‹¤ë¦„

**í•´ê²°ì±…**:
```python
def process_image(image, prompt_data, format_type):
    target_min_resolution = 2500  # ë¯¸ë¦¬ìº”ë²„ìŠ¤ ê¸°ì¤€
    width, height = image.size
    
    if width < target_min_resolution or height < target_min_resolution:
        scale = max(target_min_resolution / width, target_min_resolution / height)
        new_width = int(width * scale)
        new_height = int(height * scale)
        # Lanczos ë¦¬ìƒ˜í”Œë§ìœ¼ë¡œ í’ˆì§ˆ ìœ ì§€
        image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
```

## ì„±ëŠ¥ ìµœì í™” ê²½í—˜

### ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”

ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ ì—°ì† ìƒì„±í•  ë•Œ ëª¨ë¸ ë¡œë”© ì‹œê°„ì„ ì¤„ì´ê¸° ìœ„í•´ ì‹±ê¸€í†¤ íŒ¨í„´ì„ ê²€í† í–ˆì§€ë§Œ, ë©”ëª¨ë¦¬ ê´€ë¦¬ì˜ ë³µì¡ì„± ë•Œë¬¸ì— ë‹¨ìˆœí•œ ë°©ì‹ì„ ìœ ì§€í–ˆìŠµë‹ˆë‹¤:

```python
# ê° ì´ë¯¸ì§€ ìƒì„± ì‹œë§ˆë‹¤ ëª¨ë¸ ë¡œë”© (ë‹¨ìˆœí•˜ì§€ë§Œ ì•ˆì „)
def generate_image_with_local(prompt_data):
    pipe = StableDiffusionXLPipeline.from_pretrained(...)
    image = pipe(prompt_data["prompt"])
    return image
```

### ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ìµœì í™”

API ëª¨ë¸ ì‚¬ìš© ì‹œ ì†ë„ ì œí•œì„ ê³ ë ¤í•œ ëŒ€ê¸° ì‹œê°„ ì¶”ê°€:

```python
# API ëª¨ë¸ ì‚¬ìš© í›„ ëŒ€ê¸°
if actual_model_used not in ("local", "flux") and i < count - 1:
    wait_time = random.randint(2, 5)
    print(f"â±ï¸ {wait_time}ì´ˆ ëŒ€ê¸° ì¤‘...")
    time.sleep(wait_time)
```

## í”„ë¡œì íŠ¸ íšŒê³  ë° ê°œì„  ë°©í–¥

### ë§Œì¡±ìŠ¤ëŸ¬ì› ë˜ ì 

1. **ë¹„ìš© íš¨ìœ¨ì„±**: ë¡œì»¬ GPU í™œìš©ìœ¼ë¡œ API ë¹„ìš© ì—†ì´ ëŒ€ëŸ‰ ìƒì„± ê°€ëŠ¥
2. **ìœ ì—°ì„±**: ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ í”„ë¡¬í”„íŠ¸ ëª¨ë“œ ì§€ì›ìœ¼ë¡œ ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ ëŒ€ì‘
3. **ìë™í™”**: í”„ë¡¬í”„íŠ¸ ìƒì„±ë¶€í„° ë©”íƒ€ë°ì´í„° ê´€ë¦¬ê¹Œì§€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ìë™í™”

### ì•„ì‰¬ì› ë˜ ì ê³¼ ê°œì„  ë°©í–¥

1. **ì†ë„**: ë¡œì»¬ ëª¨ë¸ì˜ ìƒì„± ì†ë„ê°€ API ëª¨ë¸ ëŒ€ë¹„ ëŠë¦¼
   - **ê°œì„  ë°©ì•ˆ**: ë” ë¹ ë¥¸ GPUë‚˜ ëª¨ë¸ ë³‘ë ¬ ì²˜ë¦¬ ê²€í† 

2. **í’ˆì§ˆ ì¼ê´€ì„±**: í”„ë¡¬í”„íŠ¸ì— ë”°ë¥¸ í’ˆì§ˆ í¸ì°¨
   - **ê°œì„  ë°©ì•ˆ**: í”„ë¡¬í”„íŠ¸ í’ˆì§ˆ ê²€ì¦ ë¡œì§ ì¶”ê°€

3. **ì—ëŸ¬ ì²˜ë¦¬**: ì˜ˆìƒì¹˜ ëª»í•œ ìƒí™©ì—ì„œì˜ ì—ëŸ¬ ì²˜ë¦¬ ë¶€ì¡±
   - **ê°œì„  ë°©ì•ˆ**: ë” ì„¸ë°€í•œ ì˜ˆì™¸ ì²˜ë¦¬ì™€ ë¡œê¹… ì‹œìŠ¤í…œ êµ¬ì¶•

### í–¥í›„ ê³„íš

1. **ì›¹ ì¸í„°í˜ì´ìŠ¤ ê°œë°œ**: ëª…ë ¹ì¤„ì´ ì•„ë‹Œ ì›¹ ê¸°ë°˜ ì¸í„°í˜ì´ìŠ¤
2. **í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ**: ìƒì„±ëœ ì´ë¯¸ì§€ì˜ í’ˆì§ˆì„ ìë™ í‰ê°€í•˜ëŠ” ëª¨ë“ˆ
3. **ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™**: ìƒì„±ëœ ì´ë¯¸ì§€ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬
4. **ControlNet í†µí•©**: ë” ì •ë°€í•œ ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•œ ControlNet ì§€ì›

---

ì´ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ë¡œì»¬ GPUì™€ ë‹¤ì–‘í•œ AI ëª¨ë¸ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¡°í•©í•˜ì—¬ ì‹¤ìš©ì ì¸ ìë™í™” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì™„ë²½í•˜ì§€ëŠ” ì•Šì§€ë§Œ, ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€ì˜ ê²°ê³¼ë¬¼ì„ ì–»ì„ ìˆ˜ ìˆì—ˆê³ , ë¬´ì—‡ë³´ë‹¤ ë¹„ìš© ë¶€ë‹´ ì—†ì´ ë‹¤ì–‘í•œ ì‹¤í—˜ì„ í•  ìˆ˜ ìˆì—ˆë˜ ì ì´ í° ìˆ˜í™•ì´ì—ˆìŠµë‹ˆë‹¤. 

í˜¹ì‹œ ë¹„ìŠ·í•œ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ì‹œëŠ” ë¶„ë“¤ê»˜ ì‘ì€ ë„ì›€ì´ ë˜ê¸¸ ë°”ë¼ë©°, ë” ì¢‹ì€ ì•„ì´ë””ì–´ë‚˜ ê°œì„  ë°©ì•ˆì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ê³µìœ í•´ ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.